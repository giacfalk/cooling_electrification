#########
prediction <- predict.rfsrc(pr, clusters)
library(tidyverse)
y = read.csv('D:/OneDrive - FONDAZIONE ENI ENRICO MATTEI/Current papers/Prod_Uses_Agriculture/PrElGen_database/processed_folder/clusters_train.csv')
y['acc_pop_share_t1'] = y['acc_pop_t1'] / (y['acc_pop_t1'] + y['acc_pop_t2'] + y['acc_pop_t3'] + y['acc_pop_t4'])
y['acc_pop_share_t2'] = y['acc_pop_t2'] / (y['acc_pop_t1'] + y['acc_pop_t2'] + y['acc_pop_t3'] + y['acc_pop_t4'])
y['acc_pop_share_t3'] = y['acc_pop_t3'] / (y['acc_pop_t1'] + y['acc_pop_t2'] + y['acc_pop_t3'] + y['acc_pop_t4'])
y['acc_pop_share_t4'] = y['acc_pop_t4'] / (y['acc_pop_t1'] + y['acc_pop_t2'] + y['acc_pop_t3'] + y['acc_pop_t4'])
y = y %>% dplyr::select('acc_pop_share_t1', 'acc_pop_share_t2', 'acc_pop_share_t3', 'acc_pop_share_t4', 'HCWIXQPLOW', 'HCWIXQP2ND', 'HCWIXQPMID', 'HCWIXQP4TH', 'HCWIXQPHGH', 'popdens', 'isurbanmaj', 'ISO') %>% as.data.frame()
y2 = y[complete.cases(y), ]
# Partition data
splitSample <- sample(1:2, size=nrow(y2), prob=c(0.8,0.2), replace = TRUE)
train.hex <- y2[splitSample==1,]
test.hex <- y2[splitSample==2,]
library(randomForestSRC)
pr = rfsrc(Multivar(acc_pop_share_t1, acc_pop_share_t2, acc_pop_share_t3, acc_pop_share_t4)~.,data = train.hex, importance=T)
print(pr)
prediction <- predict.rfsrc(pr, test.hex)
##########
clusters = read.csv('D:/OneDrive - FONDAZIONE ENI ENRICO MATTEI/Current papers/Prod_Uses_Agriculture/PrElGen_database/processed_folder/clusters_predict.csv')
clusters$popdens = clusters$popsum/clusters$Area
clusters$ISO = as.factor("KE")
clusters = clusters %>% dplyr::select('HCWIXQPLOW', 'HCWIXQP2ND', 'HCWIXQPMID', 'HCWIXQP4TH', 'HCWIXQPHGH', 'popdens', 'id', 'ISO') %>% as.data.frame()
#
clusters =clusters[complete.cases(clusters), ]
#########
#########
prediction <- predict.rfsrc(pr, clusters)
##########
clusters = read.csv('D:/OneDrive - FONDAZIONE ENI ENRICO MATTEI/Current papers/Prod_Uses_Agriculture/PrElGen_database/processed_folder/clusters_predict.csv')
clusters = clusters %>% dplyr::select('HCWIXQPLOW', 'HCWIXQP2ND', 'HCWIXQPMID', 'HCWIXQP4TH', 'HCWIXQPHGH', 'popdens', 'id', 'ISO', 'isurbanmajority') %>% as.data.frame()
clusters$isurbanmaj <- clusters$isurbanmajority
#
clusters =clusters[complete.cases(clusters), ]
#########
prediction <- predict.rfsrc(pr, clusters)
clusters2
clusters$acc_pop_share_t1_new = prediction$regrOutput$acc_pop_share_t1$predicted
clusters$acc_pop_share_t2_new = prediction$regrOutput$acc_pop_share_t2$predicted
clusters$acc_pop_share_t3_new = prediction$regrOutput$acc_pop_share_t3$predicted
clusters$acc_pop_share_t4_new = prediction$regrOutput$acc_pop_share_t4$predicted
clusters = clusters %>% dplyr::select(-HCWIXQPLOW, -HCWIXQP2ND, -HCWIXQPMID, -HCWIXQP4TH, -HCWIXQPHGH) %>% as.data.frame()
clusters$id = as.character(clusters$id)
clusters2 = read.csv('D:/OneDrive - FONDAZIONE ENI ENRICO MATTEI/Current papers/Prod_Uses_Agriculture/PrElGen_database/processed_folder/clusters_predict.csv')
clusters2$id = as.character(clusters2$id)
clusters2 = merge(clusters, clusters2, by="id")
#
clusters2$id = as.character(clusters2$id)
clusters$acc_pop_share_t1_new
clusters$acc_pop_share_t1_new+clusters$acc_pop_share_t2_new+clusters$acc_pop_share_t3_new+clusters$acc_pop_share_t4_new
load("D:/data.RData")
grd$Residential
summary(grd$Residential)
sf$PerHHD_tt
sf_residential$value
(sf_residential$value)
sf$PerHHD_tt
summary(sf$PerHHD_tt)
sf$sf_residential_tt
(sf$sf_residential_tt * 1000)/(as.numeric(sf$HHs) * (sf$noaccsum/sf$popsum))
summary((sf$sf_residential_tt * 1000)/(as.numeric(sf$HHs) * (sf$noaccsum/sf$popsum)))
sf$isurbanmajority
group_by(sf, isurbanmajority) %>% mutate(a=(sf_residential_tt * 1000)/(as.numeric(HHs) * (sf$noaccsum/popsum)))
library(tidyverse)
group_by(sf, isurbanmajority) %>% mutate(a=(sf_residential_tt * 1000)/(as.numeric(HHs) * (sf$noaccsum/popsum)))
group_by(sf, isurbanmajority) %>% mutate(a=(sf_residential_tt * 1000)/(as.numeric(HHs) * (noaccsum/popsum)))
group_by(sf, isurbanmajority) %>% mutate(a=(sf_residential_tt * 1000)/(as.numeric(HHs) * (noaccsum/popsum))) %>% select(a)
group_by(sf, isurbanmajority) %>% mutate(a=(sf_residential_tt * 1000)/(as.numeric(HHs) * (noaccsum/popsum))) %>% select(a, -geometry)
group_by(sf, isurbanmajority) %>% summarise(mean(sf_residential_tt * 1000)/(as.numeric(HHs) * (noaccsum/popsum))) %>% select(a, -geometry)
group_by(sf, isurbanmajority) %>% summarise(mean(sf_residential_tt * 1000)/(as.numeric(HHs) * (noaccsum/popsum)))
group_by(sf, isurbanmajority) %>% summarise(a=mean(sf_residential_tt * 1000)/(as.numeric(HHs) * (noaccsum/popsum)))
group_by(sf, isurbanmajority) %>% mutate(a=(sf_residential_tt * 1000)/(as.numeric(HHs) * (noaccsum/popsum))) %>% summarise(a=mean(a, na.rm=T))
group_by(sf, isurbanmajority) %>% mutate(a=(ifelse(sf_residential_tt>1, sf_residential_tt, NA) * 1000)/(as.numeric(HHs) * (noaccsum/popsum))) %>% summarise(a=mean(a, na.rm=T))
0.4/1.5
remotes::install_github("lolow/gdxtools")
Yvar <- c(8,9,10,13,12, 14,18,12,8,9,   1,3,2,3,4)
#To generate a single bootstrap sample
sample(Yvar, replace = TRUE)
#To generate a single bootstrap sample
sample(Yvar, replace = TRUE)
#generate 1000 bootstrap samples
boot <-list()
for (i in 1:1000)
#generate 1000 bootstrap samples
boot <-list()
for (i in 1:1000){
boot[[i]] <- sample(Yvar,replace=TRUE)
}
Yvar <- c(8,9,10,13,12, 14,18,12,8,9,   1,3,2,3,4)
Xvar <- c(rep("A", 5),  rep("B", 5),    rep("C", 5))
mydf <- data.frame (Yvar, Xvar)
boot.samples <- list()
for(i in 1:10) {
b.samples.cases <- sample(length(Xvar), length(Xvar), replace=TRUE)
b.mydf <- mydf[b.samples.cases,]
boot.samples[[i]] <- b.mydf
}
str(boot.samples)
boot.samples[1]
boot.samples
str(boot.samples)
boot.samples[1]
Yvar <- c(8,9,10,13,12, 14,18,12,8,9,   1,3,2,3,4)
# parameters for Yvar
mean.y <- mean(Yvar)
sd.y <- sd(Yvar)
#To generate a single bootstrap sample with assumed normal distribution (mean, sd)
rnorm(length(Yvar), mean.y, sd.y)
#generate 1000 bootstrap samples
boot <-list()
for (i in 1:1000)
boot[[i]] <- rnorm(length(Yvar), mean.y, sd.y)
jackdf <- list()
jack <- numeric(length(Yvar)-1)
for (i in 1:length (Yvar)){
for (j in 1:length(Yvar)){
if(j < i){
jack[j] <- Yvar[j]
}  else if(j > i) {
jack[j-1] <- Yvar[j]
}
}
jackdf[[i]] <- jack
}
jackdf
#testing dataset
mydf.test <- mydf[test.id]
Yvar <- c(8,9,10,13,12, 14,18,12,8,9,   1,3,2,3,4)
Xvar <- c(rep(1, 5),  rep(2, 5),    rep(3, 5))
mydf <- data.frame (Yvar, Xvar)
training.id <- sample(1:nrow(mydf), round(nrow(mydf)/2,0), replace = FALSE)
test.id <- setdiff(1:nrow(mydf), training.id)
# training dataset
mydf.train <- mydf[training.id]
#testing dataset
mydf.test <- mydf[test.id]
Yvar <- c(8,9,10,13,12, 14,18,12,8,9,   1,3,2,3,4)
Xvar <- c(rep(1, 5),  rep(2, 5),    rep(3, 5))
mydf <- data.frame (Yvar, Xvar)
training.id <- sample(1:nrow(mydf), round(nrow(mydf)/2,0), replace = FALSE)
test.id <- setdiff(1:nrow(mydf), training.id)
# training dataset
mydf.train <- mydf[training.id]
#testing dataset
mydf.test <- mydf[test.id]
file.path(Sys.getenv("USERPROFILE"),"Desktop")
load("D:/OneDrive - FONDAZIONE ENI ENRICO MATTEI/Current papers/Prod_Uses_Agriculture/Repo/residual_productive.r")
desk_path <- file.path(Sys.getenv("USERPROFILE"),"Desktop")
home_repo_folder <- read.table(paste0(desk_path, "/repo_folder_path.txt"),header = F,nrows = 1)
## residual productive demand
library(sf)
library(tidyverse)
library(raster)
library(rgeos)
desk_path <- file.path(Sys.getenv("USERPROFILE"),"Desktop")
home_repo_folder <- read.table(paste0(desk_path, "/repo_folder_path.txt"),header = F,nrows = 1)
db_folder <- read.table(paste0(desk_path, "/repo_folder_path.txt"),header = F,nrows = 1)
home_repo_folder <- "D:/OneDrive - FONDAZIONE ENI ENRICO MATTEI/Current papers/Prod_Uses_Agriculture/Repo"
db_folder <- "D:/OneDrive - FONDAZIONE ENI ENRICO MATTEI/Current papers/Prod_Uses_Agriculture/MLED_database"
sf <- read_sf(paste0(db_folder, '/processed_folder/clusters_16.gpkg'))
a <- read.csv(paste0(db_folder, '/processed_folder/clusters_8.csv') %>% dplyr::select(starts_with("kwh_cropproc_tt"), id))
a <- read.csv(paste0(db_folder, '/processed_folder/clusters_8.csv')
a <- read.csv(paste0(db_folder, '/processed_folder/clusters_8.csv')) %>% dplyr::select(starts_with("kwh_cropproc_tt"), id)
b<- read.csv(paste0(db_folder, '/processed_folder/clusters_16.csv'))
a$X=NULL
b$X=NULL
sf = merge(sf, a, by="id")
sf = merge(sf, b, by="id")
# calculate paved road density in each cluster
roads<-read_sf(paste0(home_repo_folder, '/onsset/input/Roads/RoadsKEN.shp'))
ints = st_intersection(roads, sf)
roadslenght = tapply(st_length(ints), ints$id,sum)
sf$roadslenght = rep(0,nrow(sf))
sf$roadslenght[match(names(roadslenght),sf$id)] = roadslenght
# calculate travel time to 50 k in each cluster
traveltime <- raster(paste0(db_folder, '/input_folder/travel.tif'))
library(exactextractr)
sf$traveltime = exact_extract(traveltime, sf, 'mean')
# calculate employment rate in each cluster
empl_wealth<-read_sf(paste0(home_repo_folder, '/jrc/wealth_employment/shps/sdr_subnational_data_dhs_2014.shp'))
sf2 = st_join(sf, empl_wealth, join = st_nn, largest=T, left=T)
# run PCA
library(FactoMineR)
library(factoextra)
sf2$popdens=sf2$popsum/sf2$Area
sf2$employment = (sf2$EMEMPLMEMC + sf2$EMEMPLWEMC)/2
data_pca = dplyr::select(sf2, HCWIXQPHGH.y, employment, popdens, traveltime)
data_pca$geometry=NULL
data_pca[] <- lapply(data_pca, function(x) {
x[is.na(x)] <- mean(x, na.rm = TRUE)
x
})
data_pca <- lapply(data_pca, function(x) round((x-min(x))/(max(x)-min(x)), 2)) %>% bind_cols()
data_pca_bk <- data_pca
data_pca <- prcomp(data_pca)
PCs <- as.data.frame(data_pca$x)
PCs$PCav <- PCs$PC1
# rescale PCA to 0.3  - 0.6 range
library(scales)
PCs$PCav <- rescale(PCs$PCav, to = c(0.6, 0.3))
# hist of variables
hist <- data.frame(data_pca_bk, PCs$PCav)
hist$PCs.PCav <- rescale(hist$PCs.PCav, to = c(0, 1))
colnames(hist) <- c("Highest wealth share", "Employment rate", "Population density", "City accessibility", "PCA")
hist <- tidyr::gather(hist, key="var", value="value", 1:5)
a <- ggplot(hist)+
geom_histogram(aes(x=value, fill=var), colour="black", lwd=0.01, binwidth = 0.1)+
facet_wrap(vars(var))+
xlab("Normalised values")+
ylab("Count")+
theme(legend.position = "none")
ggsave("pca.png", a, device = "png")
sf_prod <- cbind(sf, PCs$PCav)
library(ggplot2)
#ggplot(sf)+
#geom_sf(aes(fill=PCs.PCav))
# import load curve of productive activities
load_curve_prod_act <- read.csv(paste0(db_folder, '/input_folder/productive profile.csv'))
# import load monthly curves of residential
sf_residential = dplyr::select(sf_prod, id, starts_with("PerHHD_")) %>% as.data.frame()
sf_residential$geometry = NULL
sf_residential$PerHHD_tt = NULL
sf_residential$PerHHD_tt_monthly_1 = as.numeric(sf_residential$PerHHD_tt_monthly_1) * as.numeric(sf$HHs) * (sf$noaccsum/sf$popsum)
sf_residential$PerHHD_tt_monthly_2 = as.numeric(sf_residential$PerHHD_tt_monthly_2) * as.numeric(sf$HHs) * (sf$noaccsum/sf$popsum)
sf_residential$PerHHD_tt_monthly_3 = as.numeric(sf_residential$PerHHD_tt_monthly_3) * as.numeric(sf$HHs) * (sf$noaccsum/sf$popsum)
sf_residential$PerHHD_tt_monthly_4 = as.numeric(sf_residential$PerHHD_tt_monthly_4) * as.numeric(sf$HHs) * (sf$noaccsum/sf$popsum)
sf_residential$PerHHD_tt_monthly_5 = as.numeric(sf_residential$PerHHD_tt_monthly_5) * as.numeric(sf$HHs) * (sf$noaccsum/sf$popsum)
sf_residential$PerHHD_tt_monthly_6 = as.numeric(sf_residential$PerHHD_tt_monthly_6) * as.numeric(sf$HHs) * (sf$noaccsum/sf$popsum)
sf_residential$PerHHD_tt_monthly_7 = as.numeric(sf_residential$PerHHD_tt_monthly_7) * as.numeric(sf$HHs) * (sf$noaccsum/sf$popsum)
sf_residential$PerHHD_tt_monthly_8 = as.numeric(sf_residential$PerHHD_tt_monthly_8) * as.numeric(sf$HHs) * (sf$noaccsum/sf$popsum)
sf_residential$PerHHD_tt_monthly_9 = as.numeric(sf_residential$PerHHD_tt_monthly_9) * as.numeric(sf$HHs) * (sf$noaccsum/sf$popsum)
sf_residential$PerHHD_tt_monthly_10 = as.numeric(sf_residential$PerHHD_tt_monthly_10) * as.numeric(sf$HHs) * (sf$noaccsum/sf$popsum)
sf_residential$PerHHD_tt_monthly_11 = as.numeric(sf_residential$PerHHD_tt_monthly_11) * as.numeric(sf$HHs) * (sf$noaccsum/sf$popsum)
sf_residential$PerHHD_tt_monthly_12 = as.numeric(sf_residential$PerHHD_tt_monthly_12) * as.numeric(sf$HHs) * (sf$noaccsum/sf$popsum)
sf_residential_tt = tidyr::gather(sf_residential %>% dplyr::select(290:301, 1), "date_tt", "value_tt", 1:12)
sf_residential_tt$month = as.numeric((sub('.*\\_', "", gsub("PerHHD_tt_monthly_", "", sf_residential_tt$date_tt))))
sf_residential = tidyr::gather(sf_residential %>% dplyr::select(2:289, 1), "date", "value", 1:288)
sf_residential$hour = as.numeric((sub('.*\\_', "", gsub("PerHHD_", "", sf_residential$date))))
sf_residential$month = as.numeric((sub('\\_.*', "", gsub("PerHHD_", "", sf_residential$date))))
sf_residential = merge(sf_residential,sf_residential_tt, by=c("id", "month"))
sf_residential$value = as.numeric(sf_residential$value_tt)*as.numeric(sf_residential$value)
sf_residential = dplyr::select(sf_residential, hour, value, month) %>% group_by(hour, month) %>% dplyr::summarise(value=sum(value, na.rm=T)) %>% ungroup()
sf_residential$hour = as.numeric(sf_residential$hour)
# calculate monthly markup at each hour, i.e. ratio with mean
sf_residential_2 = group_by(sf_residential, hour) %>% mutate(media = mean(value, na.rm=T)) %>% ungroup() %>% group_by(month) %>% mutate(value = value/media)
# use calculated ratios to rescale load curve of productive activities for each monthly
load_curve_prod_act <- merge(sf_residential_2, load_curve_prod_act,  by.y="ï..Hour", by.x="hour", all.x=T)
load_curve_prod_act$load_curve <- load_curve_prod_act$value * load_curve_prod_act$Share
# multiply daily total residential consumption of each month by the monthly load curves for productive
for (m in c(1:12)){
for (h in c(0:23)){
sf_prod <- mutate(sf_prod, !!paste0("residual_productive_", m, "_", h) := as.numeric(load_curve_prod_act$load_curve[load_curve_prod_act$hour == h & load_curve_prod_act$month == m]))
}}
for (m in c(1:12)){
a<-dplyr::select(sf_prod, starts_with("PerHHD_tt_monthly"))[,m] %>% as.data.frame() %>% mutate(geometry=NULL) %>% mutate_all(., .funs = as.numeric) *  sf_prod$PCs.PCav
sf_prod <- mutate(sf_prod, !!paste0("residual_productive_tt_", m) := a[,1])
}
sf <- sf_prod
###
View(sf_residential_tt)
View(sf_residential_tt)
View(sf_residential_tt)
## residual productive demand
library(sf)
library(tidyverse)
library(raster)
library(rgeos)
desk_path <- file.path(Sys.getenv("USERPROFILE"),"Desktop")
home_repo_folder <- read.table(paste0(desk_path, "/repo_folder_path.txt"),header = F,nrows = 1)
db_folder <- read.table(paste0(desk_path, "/repo_folder_path.txt"),header = F,nrows = 1)
home_repo_folder <- "D:/OneDrive - FONDAZIONE ENI ENRICO MATTEI/Current papers/Prod_Uses_Agriculture/Repo"
db_folder <- "D:/OneDrive - FONDAZIONE ENI ENRICO MATTEI/Current papers/Prod_Uses_Agriculture/MLED_database"
sf <- read_sf(paste0(db_folder, '/processed_folder/clusters_16.gpkg'))
a <- read.csv(paste0(db_folder, '/processed_folder/clusters_8.csv')) %>% dplyr::select(starts_with("kwh_cropproc_tt"), id)
b<- read.csv(paste0(db_folder, '/processed_folder/clusters_16.csv'))
a$X=NULL
b$X=NULL
sf = merge(sf, a, by="id")
sf = merge(sf, b, by="id")
# calculate paved road density in each cluster
roads<-read_sf(paste0(home_repo_folder, '/onsset/input/Roads/RoadsKEN.shp'))
ints = st_intersection(roads, sf)
roadslenght = tapply(st_length(ints), ints$id,sum)
sf$roadslenght = rep(0,nrow(sf))
sf$roadslenght[match(names(roadslenght),sf$id)] = roadslenght
# calculate travel time to 50 k in each cluster
traveltime <- raster(paste0(db_folder, '/input_folder/travel.tif'))
library(exactextractr)
sf$traveltime = exact_extract(traveltime, sf, 'mean')
# calculate employment rate in each cluster
empl_wealth<-read_sf(paste0(home_repo_folder, '/jrc/wealth_employment/shps/sdr_subnational_data_dhs_2014.shp'))
sf2 = st_join(sf, empl_wealth, join = st_nn, largest=T, left=T)
# run PCA
library(FactoMineR)
library(factoextra)
sf2$popdens=sf2$popsum/sf2$Area
sf2$employment = (sf2$EMEMPLMEMC + sf2$EMEMPLWEMC)/2
data_pca = dplyr::select(sf2, HCWIXQPHGH.y, employment, popdens, traveltime)
data_pca$geometry=NULL
data_pca[] <- lapply(data_pca, function(x) {
x[is.na(x)] <- mean(x, na.rm = TRUE)
x
})
data_pca <- lapply(data_pca, function(x) round((x-min(x))/(max(x)-min(x)), 2)) %>% bind_cols()
data_pca_bk <- data_pca
data_pca <- prcomp(data_pca)
PCs <- as.data.frame(data_pca$x)
PCs$PCav <- PCs$PC1
# rescale PCA to 0.3  - 0.6 range
library(scales)
PCs$PCav <- rescale(PCs$PCav, to = c(0.6, 0.3))
# hist of variables
hist <- data.frame(data_pca_bk, PCs$PCav)
hist$PCs.PCav <- rescale(hist$PCs.PCav, to = c(0, 1))
colnames(hist) <- c("Highest wealth share", "Employment rate", "Population density", "City accessibility", "PCA")
hist <- tidyr::gather(hist, key="var", value="value", 1:5)
sf_prod <- cbind(sf, PCs$PCav)
library(ggplot2)
load_curve_prod_act <- read.csv(paste0(db_folder, '/input_folder/productive profile.csv'))
# import load monthly curves of residential
sf_residential = dplyr::select(sf_prod, id, starts_with("PerHHD_")) %>% as.data.frame()
sf_residential$geometry = NULL
sf_residential$PerHHD_tt = NULL
sf_residential$PerHHD_tt_monthly_1 = as.numeric(sf_residential$PerHHD_tt_monthly_1) * as.numeric(sf$HHs) * (sf$noaccsum/sf$popsum)
sf_residential$PerHHD_tt_monthly_2 = as.numeric(sf_residential$PerHHD_tt_monthly_2) * as.numeric(sf$HHs) * (sf$noaccsum/sf$popsum)
sf_residential$PerHHD_tt_monthly_3 = as.numeric(sf_residential$PerHHD_tt_monthly_3) * as.numeric(sf$HHs) * (sf$noaccsum/sf$popsum)
sf_residential$PerHHD_tt_monthly_4 = as.numeric(sf_residential$PerHHD_tt_monthly_4) * as.numeric(sf$HHs) * (sf$noaccsum/sf$popsum)
sf_residential$PerHHD_tt_monthly_5 = as.numeric(sf_residential$PerHHD_tt_monthly_5) * as.numeric(sf$HHs) * (sf$noaccsum/sf$popsum)
sf_residential$PerHHD_tt_monthly_6 = as.numeric(sf_residential$PerHHD_tt_monthly_6) * as.numeric(sf$HHs) * (sf$noaccsum/sf$popsum)
sf_residential$PerHHD_tt_monthly_7 = as.numeric(sf_residential$PerHHD_tt_monthly_7) * as.numeric(sf$HHs) * (sf$noaccsum/sf$popsum)
sf_residential$PerHHD_tt_monthly_8 = as.numeric(sf_residential$PerHHD_tt_monthly_8) * as.numeric(sf$HHs) * (sf$noaccsum/sf$popsum)
sf_residential$PerHHD_tt_monthly_9 = as.numeric(sf_residential$PerHHD_tt_monthly_9) * as.numeric(sf$HHs) * (sf$noaccsum/sf$popsum)
sf_residential$PerHHD_tt_monthly_10 = as.numeric(sf_residential$PerHHD_tt_monthly_10) * as.numeric(sf$HHs) * (sf$noaccsum/sf$popsum)
sf_residential$PerHHD_tt_monthly_11 = as.numeric(sf_residential$PerHHD_tt_monthly_11) * as.numeric(sf$HHs) * (sf$noaccsum/sf$popsum)
sf_residential$PerHHD_tt_monthly_12 = as.numeric(sf_residential$PerHHD_tt_monthly_12) * as.numeric(sf$HHs) * (sf$noaccsum/sf$popsum)
sf_residential_tt = tidyr::gather(sf_residential %>% dplyr::select(290:301, 1), "date_tt", "value_tt", 1:12)
sf_residential_tt$month = as.numeric((sub('.*\\_', "", gsub("PerHHD_tt_monthly_", "", sf_residential_tt$date_tt))))
sf_residential = tidyr::gather(sf_residential %>% dplyr::select(2:289, 1), "date", "value", 1:288)
sf_residential$hour = as.numeric((sub('.*\\_', "", gsub("PerHHD_", "", sf_residential$date))))
sf_residential$month = as.numeric((sub('\\_.*', "", gsub("PerHHD_", "", sf_residential$date))))
sf_residential = merge(sf_residential,sf_residential_tt, by=c("id", "month"))
save.image("~/data.RData")
dhs <- read_sf("D:/OneDrive - FONDAZIONE ENI ENRICO MATTEI/Su-Min/Data for regression/sdr_subnational_data_2020-07-31/sdr_exports.gdb")
library(sf)
library(raster)
dhs <- read_sf("D:/OneDrive - FONDAZIONE ENI ENRICO MATTEI/Su-Min/Data for regression/sdr_subnational_data_2020-07-31/sdr_exports.gdb")
template <- raster('D:/OneDrive - FONDAZIONE ENI ENRICO MATTEI/Current papers/Anteneh/nodatamask_1.ASC')
template_pol <- rasterToPolygons(template)
library(fasterize)
colnames(dhs)
female_edu <- fasterize(dhs, template, field = "EDEDATWCSC" , fun="mean")
?fasterize
female_edu <- fasterize(dhs, template, field = "EDEDATWCSC" , fun="first")
female_edu <- fasterize(dhs, template, field = "EDEDATWCSC" , fun="first")
q1 <- fasterize(dhs, template, field = "HCWIXQPLOW" , fun="first")
q2 <- fasterize(dhs, template, field = "HCWIXQP2ND" , fun="first")
q3 <- fasterize(dhs, template, field = "HCWIXQPMID" , fun="first")
q4 <- fasterize(dhs, template, field = "HCWIXQP4TH" , fun="first")
q5 <- fasterize(dhs, template, field = "HCWIXQPHGH" , fun="first")
female_household_head <- fasterize(dhs, template, field = "HCHHHDHFEM" , fun="first")
template_pol$female_edu <- exactextractr::exact_extract(female_edu, template_pol, fun="mean")
template_pol <- st_as_sf(template_pol)
template_pol$female_edu <- exactextractr::exact_extract(female_edu, template_pol, fun="mean")
# Wrapper for:
# Latent residential air cooling demand and universal household electrification
# Giacomo Falchetta
# 16/07/2020
# install.packages(c("tidyverse", "reshape2", "lubridate", "raster", "sf", "exactextractr", "countrycode", "rasterVis", "maps", "mapdata", "maptools", "rgdal", "gglorenz", "fasterize", "viridis", "data.table", "oce", "osc", "lutz", "suncalc", "rstudioapi", "cowplot"))
library(tidyverse)
library(reshape2)
library(lubridate)
library(raster)
library(sf)
library(exactextractr)
library(countrycode)
library(rasterVis)
library(maps)
library(mapdata)
library(maptools)
library(rgdal)
library(gglorenz)
library(fasterize)
library(viridis)
library(data.table)
library(oce)
library(osc)
library(lutz)
library(suncalc)
library(rstudioapi)
library(cowplot)
setwd('D:/OneDrive - FONDAZIONE ENI ENRICO MATTEI/Current papers/Latent demand air cooling/cooling_electricity_SSA')
kwh_sens <- list.files(path = 'D:/OneDrive - FONDAZIONE ENI ENRICO MATTEI/Current papers/Latent demand air cooling/cooling_electricity_SSA/sensitivity_kwh/tbase', full.names = T)
kwh_sens <- lapply(kwh_sens, read.csv)
kwh_sens <- data.table::rbindlist(kwh_sens, idcol = T)
kwh_sens$Scenario=NA
kwh_sens$Scenario <- ifelse(kwh_sens$.id==1, "22", kwh_sens$Scenario)
kwh_sens$Scenario <- ifelse(kwh_sens$.id==2, "24", kwh_sens$Scenario)
kwh_sens$Scenario <- ifelse(kwh_sens$.id==3, "26", kwh_sens$Scenario)
kwh_sens$Scenario <- ifelse(kwh_sens$.id==4, "28", kwh_sens$Scenario)
a =ggplot(kwh_sens)+
theme_classic()+
geom_bar(data = kwh_sens, aes(x = id, y = twh, fill=Scenario), stat = "sum", position = "dodge", show.legend=c(size=FALSE)) +
theme(axis.text.x = element_text(angle = 90, size=8), plot.title = element_text(hjust = 0.5))+
scale_fill_brewer(name="Tbase", palette = "Set1")+
ylab('TWh/year')+
xlab("Warming scenario")+
facet_wrap(~ scenario, ncol=2)
a
kwh_sens$id
labels(kwh_sens$id)
levels(kwh_sens$id)
levels(kwh_sens$id) <- c("Baseline", "SSP245", "SSP370")
levels(kwh_sens$scenario)
levels(kwh_sens$scenario) <- c("Tech. adopt. S1", "Tech. adopt. S2", "Tech. adopt. S3", "Tech. adopt. S4")
a =ggplot(kwh_sens)+
theme_classic()+
geom_bar(data = kwh_sens, aes(x = id, y = twh, fill=Scenario), stat = "sum", position = "dodge", show.legend=c(size=FALSE)) +
theme(axis.text.x = element_text(angle = 90, size=8), plot.title = element_text(hjust = 0.5))+
scale_fill_brewer(name="Tbase", palette = "Set1")+
ylab('TWh/year')+
xlab("Warming scenario")+
facet_wrap(~ scenario, ncol=2)
a
a =ggplot(kwh_sens)+
theme_classic()+
geom_bar(data = kwh_sens, aes(x = id, y = twh, fill=Scenario), stat = "sum", position = "dodge", show.legend=c(size=FALSE)) +
theme(axis.text.x = element_text(angle = 90, size=8), plot.title = element_text(hjust = 0.5))+
scale_fill_brewer(name="Tbase", palette = "Set1")+
ylab('TWh/year')+
xlab("Warming scenario")+
facet_wrap(~ scenario, ncol=2)+
ggtitle("Constant EERs: 2.9 urban, 2.2 rural")
kwh_sens <- list.files(path = 'D:/OneDrive - FONDAZIONE ENI ENRICO MATTEI/Current papers/Latent demand air cooling/cooling_electricity_SSA/sensitivity_kwh/eer', full.names = T)
kwh_sens <- lapply(kwh_sens, read.csv)
kwh_sens <- data.table::rbindlist(kwh_sens, idcol = T)
kwh_sens$Scenario=NA
kwh_sens$Scenario <- ifelse(kwh_sens$.id==1, "2.2U, 2R", kwh_sens$Scenario)
kwh_sens$Scenario <- ifelse(kwh_sens$.id==2, "2.9U, 2.2R", kwh_sens$Scenario)
kwh_sens$Scenario <- ifelse(kwh_sens$.id==3, "3.2U, 2.9R", kwh_sens$Scenario)
b =ggplot(kwh_sens)+
theme_classic()+
geom_bar(data = kwh_sens, aes(x = id, y = twh, fill=Scenario), stat = "sum", position = "dodge", show.legend=c(size=FALSE)) +
theme(axis.text.x = element_text(angle = 90, size=8), plot.title = element_text(hjust = 0.5))+
scale_fill_brewer(name="EERs", palette = "Set1")+
ylab('TWh/year')+
xlab("Warming scenario")+
facet_wrap(~ scenario, ncol=2)+
ggtitle("Constant Tbase 26 C°")
cowplot::plot_grid(a, b, ncol = 1, labels = "AUTO")
levels(kwh_sens$id) <- c("Baseline", "SSP245", "SSP370")
levels(kwh_sens$scenario) <- c("Tech. adopt. S1", "Tech. adopt. S2", "Tech. adopt. S3", "Tech. adopt. S4")
b =ggplot(kwh_sens)+
theme_classic()+
geom_bar(data = kwh_sens, aes(x = id, y = twh, fill=Scenario), stat = "sum", position = "dodge", show.legend=c(size=FALSE)) +
theme(axis.text.x = element_text(angle = 90, size=8), plot.title = element_text(hjust = 0.5))+
scale_fill_brewer(name="EERs", palette = "Set1")+
ylab('TWh/year')+
xlab("Warming scenario")+
facet_wrap(~ scenario, ncol=2)+
ggtitle("Constant Tbase 26 C°")
cowplot::plot_grid(a, b, ncol = 1, labels = "AUTO")
b =ggplot(kwh_sens)+
theme_classic()+
geom_bar(data = kwh_sens, aes(x = id, y = twh, fill=Scenario), stat = "sum", position = "dodge", show.legend=c(size=FALSE)) +
theme(axis.text.x = element_text(angle = 90, size=8), plot.title = element_text(hjust = 0.5))+
scale_fill_brewer(name="EERs", palette = "Set1")+
ylab('TWh/year')+
xlab("Warming scenario")+
facet_wrap(~ scenario, ncol=2)+
ggtitle("Constant Tbase: 26 C°")
cowplot::plot_grid(a, b, ncol = 1, labels = "AUTO")
ggsave("kwh.png", last_plot(), scale=1.5, height = 5, width = 4)
##
co2_sens <- list.files(path = 'D:/OneDrive - FONDAZIONE ENI ENRICO MATTEI/Current papers/Latent demand air cooling/cooling_electricity_SSA/sensitivity_co2/tbase', full.names = T)
co2_sens <- lapply(co2_sens, read.csv)
co2_sens <- data.table::rbindlist(co2_sens, idcol = T)
co2_sens$Scenario=NA
co2_sens$Scenario <- ifelse(co2_sens$.id==1, "22", co2_sens$Scenario)
co2_sens$Scenario <- ifelse(co2_sens$.id==2, "24", co2_sens$Scenario)
co2_sens$Scenario <- ifelse(co2_sens$.id==3, "26", co2_sens$Scenario)
co2_sens$Scenario <- ifelse(co2_sens$.id==4, "28", co2_sens$Scenario)
levels(co2_sens$id) <- c("Baseline", "SSP245", "SSP370")
levels(co2_sens$scenario) <- c("Tech. adopt. S1", "Tech. adopt. S2", "Tech. adopt. S3", "Tech. adopt. S4")
c =ggplot(co2_sens)+
theme_classic()+
geom_bar(data = co2_sens, aes(x = id, y = co2, fill=Scenario), stat = "sum", position = "dodge", show.legend=c(size=FALSE)) +
theme(axis.text.x = element_text(angle = 90, size=8), plot.title = element_text(hjust = 0.5))+
scale_fill_brewer(name="Tbase", palette = "Set1")+
ylab("Mt CO2/year")+
xlab("Warming scenario")+
facet_wrap(~ scenario, ncol=2)+
ggtitle("Constant EERs: 2.9 urban, 2.2 rural")
co2_sens <- list.files(path = 'D:/OneDrive - FONDAZIONE ENI ENRICO MATTEI/Current papers/Latent demand air cooling/cooling_electricity_SSA/sensitivity_co2/eer', full.names = T)
co2_sens <- lapply(co2_sens, read.csv)
co2_sens <- data.table::rbindlist(co2_sens, idcol = T)
co2_sens$Scenario=NA
co2_sens$Scenario <- ifelse(co2_sens$.id==1, "2.2U, 2R", co2_sens$Scenario)
co2_sens$Scenario <- ifelse(co2_sens$.id==2, "2.9U, 2.2R", co2_sens$Scenario)
co2_sens$Scenario <- ifelse(co2_sens$.id==3, "3.2U, 2.9R", co2_sens$Scenario)
levels(co2_sens$id) <- c("Baseline", "SSP245", "SSP370")
levels(co2_sens$scenario) <- c("Tech. adopt. S1", "Tech. adopt. S2", "Tech. adopt. S3", "Tech. adopt. S4")
d =ggplot(co2_sens)+
theme_classic()+
geom_bar(data = co2_sens, aes(x = id, y = co2, fill=Scenario), stat = "sum", position = "dodge", show.legend=c(size=FALSE)) +
theme(axis.text.x = element_text(angle = 90, size=8), plot.title = element_text(hjust = 0.5))+
scale_fill_brewer(name="EERs", palette = "Set1")+
ylab("Mt CO2/year")+
xlab("Warming scenario")+
facet_wrap(~ scenario, ncol=2)+
ggtitle("Constant Tbase: 26 C°")
cowplot::plot_grid(c, d, ncol = 1, labels = "AUTO")
ggsave("co2.png", last_plot(), scale=1.5, height = 5, width = 4)
